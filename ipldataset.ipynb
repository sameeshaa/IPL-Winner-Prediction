{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8637500,"sourceType":"datasetVersion","datasetId":990900}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n#df = pd.read_csv()\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n##finding available datasets\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loading the dataset\ndf = pd.read_csv('/kaggle/input/ipl-complete-dataset-20082020/matches.csv')\n\n#display first few rows\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#display all column names\nlist(df.columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#checking data types of columns\ndf.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#checking data values\ndf.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe(include='object')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataset dimension - no of rows, columns\ndf.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#finding how many missing values exist for each column\ndf.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#percentage of missing values per columns\ndf.isna().sum()/len(df)*100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dropping uneccessary columns \ndf.drop(['match_type','target_overs','method'],axis=1,inplace=True)\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(['date','umpire1','umpire2','target_runs','result_margin'],axis=1,inplace=True)\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#encoding categorical values  \nfrom sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder()\n\ncategorical_cols = ['toss_decision', 'city', 'player_of_match', 'venue', \n                    'team1', 'team2', 'toss_winner', 'winner', 'result', 'super_over']\n\nfor col in categorical_cols:\n    df[col] = label_encoder.fit_transform(df[col])\n    \ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"toss_win_impact = df.groupby('toss_winner')['winner'].value_counts(normalize=True).get(1,0)\ndf['toss_win_impact'] = df['toss_winner'].map(toss_win_impact)\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer \n\nimputer_toss = SimpleImputer(strategy='most_frequent')\ndf['toss_winner'] = imputer_toss.fit_transform(df[['toss_winner']])\n\nprint(df['toss_winner'].isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder()\n\ndf['toss_win_impact'] = label_encoder.fit_transform(df['toss_win_impact'])\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#spliting the dataset to features and trget variable\nfeature_cols = ['team1','team2','toss_winner','toss_decision','result']\nX = df[feature_cols]\ny = df.winner #target variable ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#scaling input variables\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX1 = scaler.fit_transform(X)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#splitting x and y into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, y_train, y_test = train_test_split(X1,y,test_size=0.25,random_state=0)\nprint('Whole Data Shape',df.shape)\nprint('X1 train shape',X1_train.shape)\nprint('x1 test shape',X1_test.shape)\n#.25 - 25% testing\n#.75 - 75% training","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [10, 20, 30, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n\ngrid_search.fit(X1_train, y_train)\n\nprint(f\"Best Hyperparameters: {grid_search.best_params_}\")\nprint(f\"Best Cross-validation Accuracy: {grid_search.best_score_}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#instantiate the model\n#creating a logistic regression classifier object \n#fitting the model on the trai set\n#perfoming predictions on pred \n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel = RandomForestClassifier(n_estimators=100,random_state=42)\nmodel.fit(X1_train,y_train)\n\ny_pred = model.predict(X1_test)\nprint(\"Random Forest Accuracy:\",accuracy_score(y_test,y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#displaying predicted values\ny_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ncm = confusion_matrix(y_test, y_pred, labels = logreg.classes_)\n\ndisp = ConfusionMatrixDisplay(cm,display_labels=logreg.classes_)\ndisp.plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import RocCurveDisplay\nLogreg_roc = RocCurveDisplay.from_estimator(logreg,X1_test,y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ndf['team1'] = label_encoder.fit_transform(df['team1'])\n\nteam_mapping = dict(zip(label_encoder.classes_,label_encoder.transform(label_encoder.classes_)))\nreverse_mapping = {v: k for k, v in team_mapping.items()}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns \nimport matplotlib.pyplot as plt\n\nteam_wins = df['winner'].value_counts()\nteam_wins.index = team_wins.index.map(reverse_mapping)\n\nplt.figure(figsize=(10,5))\nsns.barplot(x=team_wins.index, y=team_wins.values)\nplt.xticks(rotation=90)\nplt.title('Win counts by team')\nplt.xlabel('Teams')\nplt.ylabel('Number of Wins')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}